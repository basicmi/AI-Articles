## Weekly Digest 2018-05 \#1

**[A Brief Guide of xPU for AI Accelerators](https://www.sigarch.org/a-brief-guide-of-xpu-for-ai-accelerators/)**
> a quick survey on various AI hardware accelerators developed in the last several years: Other than CPU/GPU that we are familiar with, we have seem many xPUs that are related to AI hardware accelerators: From A to Z, which letter is not used yet for your xPU design? 

**[Introduction to Decision Tree Learning](https://heartbeat.fritz.ai/introduction-to-decision-tree-learning-cd604f85e236)**
> From Kaggle to classrooms, one of the first lessons in machine learning involves decision trees. The reason for the focus on decision trees is that they aren’t very mathematics heavy compared to other ML approaches, and at the same time, they provide reasonable accuracy on classification problems.

**[Lessons from My First Two Years of AI Research](http://web.mit.edu/tslvr/www/lessons_two_years.html)**
> "A friend of mine who is about to start a career in artificial intelligence research recently asked what I wish I had known when I started two years ago. Below are some lessons I have learned so far. They range from general life lessons to relatively specific tricks of the AI trade. I hope others find them useful."

**[Comparing Google’s TPUv2 against Nvidia’s V100 on ResNet-50](https://blog.riseml.com/comparing-google-tpuv2-against-nvidia-v100-on-resnet-50-c2bbb6a51e5e)**
> Google recently added the [Tensor Processing Unit v2 (TPUv2)](https://cloudplatform.googleblog.com/2018/02/Cloud-TPU-machine-learning-accelerators-now-available-in-beta.html), a custom-developed microchip to accelerate deep learning, to its cloud offering. The TPUv2 is the second generation of this chip and the first publicly available deep learning accelerator that has the potential of becoming an alternative to Nvidia GPUs. We recently reported our first experience and received a lot of requests for a more detailed comparison to [Nvidia V100 GPUs](https://www.nvidia.com/en-us/data-center/tesla-v100/).

**[UP NEXT: A BETTER RECOMMENDATION SYSTEM](https://www.wired.com/story/creating-ethical-recommendation-engines/)**
> As the consequences of curatorial decisions grow more dire, we need to ask: Can we make the internet’s recommendation engines more ethical? And if so, how?

**[Can Amazon Build a Home Robot That Is Useful and Affordable?](https://spectrum.ieee.org/automaton/robotics/home-robots/amazon-home-robots)**
> Domestic robots are hard, but rumor has it that Amazon wants in.

**[The economics of artificial intelligence](https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/the-economics-of-artificial-intelligence)**
> Rotman School of Management professor Ajay Agrawal explains how AI changes the cost of prediction and what this means for business.

**[Toward the Jet Age of machine learning](https://www.oreilly.com/ideas/toward-the-jet-age-of-machine-learning)**
> Solving the challenges of efficiency, automation, and safety will require cooperation between researchers and engineers spanning both academia and industry.

**[Why Is the Human Brain So Efficient?](http://nautil.us/issue/59/connections/why-is-the-human-brain-so-efficient)**
> How massive parallelism lifts the brain’s performance above that of AI.

**[How Artificial Intelligence Could Increase the Risk of Nuclear War](https://www.rand.org/blog/articles/2018/04/how-artificial-intelligence-could-increase-the-risk.html)**
> Could artificial intelligence upend concepts of nuclear deterrence that have helped spare the world from nuclear war since 1945? Stunning advances in AI—coupled with a proliferation of drones, satellites, and other sensors—raise the possibility that countries could find and threaten each other's nuclear forces, escalating tensions.

**[AI Software Writing AI Software For Healthcare?](https://www.nextplatform.com/2018/04/26/ai-software-writing-ai-software-for-healthcare/)**
> Software that writes itself? We needed to think about that comment some more, how does this help healthcare? Is the software that the software writes going to be the right software? So many questions.

**[Accelerating Deep Neuroevolution: Train Atari in Hours on a Single Personal Computer](https://eng.uber.com/accelerated-neuroevolution/)**
> we are [releasing open source code](https://github.com/uber-common/deep-neuroevolution/tree/master/gpu_implementation) that makes it possible to conduct such research much faster and cheaper. With this code, the time it takes to train deep neural networks to play Atari, which takes ~1 hour on 720 CPUs, now takes ~4 hours on a single modern desktop. This point is important because it dramatically affects our perceptions of the range of resources required to conduct this kind of research, making it accessible to a much larger group of researchers.
