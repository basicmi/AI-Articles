## Weekly Digest 2017-12 \#2

**[Optimization for Deep Learning Highlights in 2017](http://ruder.io/deep-learning-optimization-2017/index.html)**
> In this blog post, I will touch on the most exciting highlights and most promising directions in optimization for Deep Learning in my opinion. Note that this blog post assumes a familiarity with SGD and with adaptive learning rate methods such as Adam. To get up to speed, refer to this blog post for an overview of [existing gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/index.html).

**[Using Artiï¬cial Intelligence to Augment Human Intelligence](https://distill.pub/2017/aia/)**
> By creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning. 
