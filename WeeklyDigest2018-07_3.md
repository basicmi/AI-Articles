# AI Articles of the week

**[Why businesses fail at machine learning](https://hackernoon.com/why-businesses-fail-at-machine-learning-fbff41c4d5db)**
> I’d like to let you in on a secret: when people say ‘machine learning’ it sounds like there’s only one discipline here. There are two, and if businesses don’t understand the difference, they can experience a world of trouble.

**[Facial recognition technology: The need for public regulation and corporate responsibility](https://blogs.microsoft.com/on-the-issues/2018/07/13/facial-recognition-technology-the-need-for-public-regulation-and-corporate-responsibility/)**
> Facial recognition technology raises issues that go to the heart of fundamental human rights protections like privacy and freedom of expression. These issues heighten responsibility for tech companies that create these products. In our view, they also call for thoughtful government regulation and for the development of norms around acceptable uses. In a democratic republic, there is no substitute for decision making by our elected representatives regarding the issues that require the balancing of public safety with the essence of our democratic freedoms. Facial recognition will require the public and private sectors alike to step up – and to act.

**[Seedbank — discover machine learning examples](https://medium.com/tensorflow/seedbank-discover-machine-learning-examples-2ff894542b57)**
> we’re launching [Seedbank](http://tools.google.com/seedbank), a place to discover interactive machine learning examples which you can run from your browser, no set-up required. 

**[Medical AI Safety: We have a problem.](https://lukeoakdenrayner.wordpress.com/2018/07/11/medical-ai-safety-we-have-a-problem/)**
> For the first time ever, AI systems could actually be responsible for medical disasters.

**[VTA: An Open, Customizable Deep Learning Acceleration Stack](https://tvm.ai/2018/07/12/vta-release-announcement.html)**
> We are excited to announce the launch of the Versatile Tensor Accelerator (VTA, pronounced vita), an open, generic, and customizable deep learning accelerator design. 

**[NLP's ImageNet moment has arrived](http://ruder.io/nlp-imagenet/)**
> Big changes are underway in the world of Natural Language Processing (NLP). The long reign of word vectors as NLP’s core representation technique has seen an exciting new line of challengers emerge: [ELMo](https://arxiv.org/abs/1802.05365), [ULMFiT](https://arxiv.org/abs/1801.06146), and the [OpenAI transformer](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf). These works [made](https://blog.openai.com/language-unsupervised/) [headlines](https://techcrunch.com/2018/06/15/machines-learn-language-better-by-using-a-deep-understanding-of-words/) by demonstrating that pretrained language models can be used to achieve state-of-the-art results on a wide range of NLP tasks. Such methods herald a watershed moment: they may have the same wide-ranging impact on NLP as pretrained ImageNet models had on computer vision.

**[Hardware Accelerators for Machine Learning (CS 217)](https://cs217.github.io/)**
> This course provides in-depth coverage of the architectural techniques used to design accelerators for training and inference in machine learning systems. 

**[Text Generation using a RNN](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/text_generation.ipynb)**
> This notebook demonstrates how to generate text using an RNN using tf.keras and eager execution. If you like, you can write a similar model using less code. Here, we show a lower-level impementation that's useful to understand as prework before diving in to deeper examples in a similar, like Neural Machine Translation with Attention.

**[What Image Classifiers Can Do About Unknown Objects](https://petewarden.com/2018/07/06/what-image-classifiers-can-do-about-unknown-objects/)**
> 

**[From shallow to deep learning in fraud](https://eng.lyft.com/from-shallow-to-deep-learning-in-fraud-9dafcbcef743)**
> A Research Scientist’s journey through hand-coded regressors, pickled trees, and attentive neural networks

**[Why Most of Us Fail to Grasp Coming Exponential Gains in AI]()**
> By now, most of us are familiar with Moore’s Law, the famous maxim that the development of computing power follows an exponential curve, doubling in price-performance (that is, speed per unit cost) every 18 months or so. When it comes to applying Moore’s Law to their own business strategies, however, even visionary thinkers frequently suffer from a giant “AI blind spot.”

**[A List of AI Chip/IP](https://basicmi.github.io/Deep-Learning-Processor-List/)**
> Latest updates
