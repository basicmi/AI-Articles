# AI Articles of the week

**[Natural Language Processing is Fun!](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e)**
> How computers understand Human Language

**[Program Synthesis in 2017-18]()**
> ...this post — a high-level overview of the recent ideas and representative papers in program synthesis as of mid-2018.

**[How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)**
> Decision trees are one of my favorite models. They are simple, and they are powerful. In fact most high performing Kaggle entries are a combination of XGBoost, which is variant of decision tree, and some very clever feature engineering.

**[Ten Techniques Learned From fast.ai](https://blog.floydhub.com/ten-techniques-from-fast-ai/)**
> Right now, Jeremy Howard – the co-founder of fast.ai – currently holds the 105th highest score for the plant seedling classification contest on Kaggle, but he's dropping fast. Why? His own students are beating him. And their names can now be found across the tops of leaderboards all over Kaggle.

**[Using Uncertainty to Interpret your Model](https://engineering.taboola.com/using-uncertainty-interpret-model/)**
> As deep neural networks (DNN) become more powerful, their complexity increases. This complexity introduces new challenges, including model interpretability.

**[When Recurrent Models Don't Need to be Recurrent](http://www.offconvex.org/2018/07/27/approximating-recurrent/)**
> We discuss several proposed answers to this question and highlight our [recent work](https://arxiv.org/abs/1805.10369) that offers an explanation in terms of a fundamental stability property.

**[L1: Tensor Studio](https://github.com/mlajtos/L1)**
> [L1: Tensor Studio](https://mlajtos.github.io/L1/latest/) is a live-programming environment for differentiable linear algebra. A playground for tensors.

**[Good First Impressions According to Data Science](https://medium.com/datadriveninvestor/good-first-impressions-according-to-data-science-499d4225044d)**
> Making a good first impression is hard. I made a model that can predict how good of an impression you are making based on a video clip submission.

**[Embrace the noise: A case study of text annotation for medical imaging](https://lighttag.io/blog/embrace-the-noise/)**
> In this post we’ll discuss the recent paper [TextRay: Mining Clinical Reports to Gain a Broad Understanding of Chest X-rays](https://arxiv.org/abs/1806.02121) focusing on the best practices the paper exemplifies with regards to labeling text data for NLP. 

**[Want Less-Biased Decisions? Use Algorithms.](https://hbr.org/2018/07/want-less-biased-decisions-use-algorithms)**
> And that is the most relevant question for practitioners and policy makers: How do the bias and performance of algorithms compare with the status quo? Rather than simply asking whether algorithms are flawed, we should be asking how these flaws compare with those of human beings.

**[Data's day of reckoning](https://www.oreilly.com/ideas/datas-day-of-reckoning)**
> We can build a future we want to live in, or we can build a nightmare. The choice is up to us.

**[Evolutionary algorithm outperforms deep-learning machines at video games](https://www.technologyreview.com/s/611568/evolutionary-algorithm-outperforms-deep-learning-machines-at-video-games/)**
> Neural networks have garnered all the headlines, but a much more powerful approach is waiting in the wings.
