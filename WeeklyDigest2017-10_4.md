## Weekly Digest 2017-10 \#4

**[A List of Chip/IP for Deep Learning (keep updating)](https://basicmi.github.io/Deep-Learning-Processor-List/)**

**1. [Intel® Nervana™ Neural Network Processors (NNP) Redefine AI Silicon](https://www.intelnervana.com/intel-nervana-neural-network-processors-nnp-redefine-ai-silicon/)**
> As our Intel CEO Brian Krzanich discussed earlier today at Wall Street Journal’s D.Live event, Intel will soon be shipping the world’s first family of processors designed from the ground up for artificial intelligence (AI): the [Intel® Nervana™ Neural Network Processor](https://newsroom.intel.com/editorials/intel-pioneers-new-technologies-advance-artificial-intelligence/) family (formerly known as “Lake Crest”). This family of processors is over 3 years in the making, and on behalf of the team building it, I’d like to share a bit more insight on the motivation and design behind the world’s first neural network processor.

**2. [Pixel Visual Core: image processing and machine learning on Pixel 2](https://www.blog.google/products/pixel/pixel-visual-core-image-processing-and-machine-learning-pixel-2/)**
> Pixel Visual Core is Google’s first custom-designed co-processor for consumer products. It’s built into every Pixel 2, and in the coming months, we’ll turn it on through a software update to enable more applications to use Pixel 2’s camera for taking HDR+ quality pictures.

**3. [Introducing NNVM Compiler: A New Open End-to-End Compiler for AI Frameworks](https://aws.amazon.com/cn/blogs/ai/introducing-nnvm-compiler-a-new-open-end-to-end-compiler-for-ai-frameworks/)**
> Diverse AI frameworks and hardware bring huge benefits to users, but it is very challenging to AI developers to deliver consistent results to end users. Luckily, we are not the first to face this kind of problems. Computer science has a long history of running various programming languages on different hardware. One key technology to solve this problem is the compiler. Motivated by the compiler technology, a group of researchers including Tianqi Chen, Thierry Moreau, Haichen Shen, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy from Paul G. Allen School of Computer Science & Engineering, University of Washington, together with Ziheng Jiang from the AWS AI team, introduced the [TVM stack](http://tvmlang.org/2017/08/17/tvm-release-announcement.html) to simplify this problem.
> Today, AWS is excited to announce, together with the research team from UW, an end-to-end compiler based on the TVM stack that compiles workloads directly from various deep learning frontends into optimized machine codes. Let’s take a look at the architecture.

**4. [Numerai’s Master Plan](https://medium.com/numerai/numerais-master-plan-1a00f133dba9)**
> The core idea of Numerai was to give away all of our data for free, and let anyone train machine learning algorithms on it and submit predictions to our hedge fund. This was a very counterintuitive idea. We already had our own internal machine learning models on the data so it seemed like a distraction to open it up to the world.
> Some of our distractions have already proven themselves to be outrageously successful, and others are still developing, but they actually aren’t distractions at all. They are all part of the plan.

**5. [Routes to Defensibility for your AI Startup](https://machinelearnings.co/routes-to-defensibility-for-your-ai-startup-2875a1b51d4e)**
> A simple framework for understanding the impact of data network effects and incumbents’ advantages in your industry
> In short: VCs are looking for companies that could be worth hundreds of millions or billions of dollars by the next 5–10 years and:
> Projected future cash flows are a proxy for valuation,
> Ability to generate profit is a proxy for projected future cash flow,
> Moats are a proxy for the ability to generate profits.

**6. [How machine learning is helping neuroscientists understand the brain](https://massivesci.com/articles/neuroscience-machine-learning-metaphors/)**
> The workings of the brain are the greatest mystery in science. Unlike our models of physics, strong enough to predict gravitational waves and unseen particles, our brain models explain only the most basic forms of perception, cognition, and behavior. We know plenty about the biology of neurons and glia, the cells that make up the brain. And we know enough about how they interact with each other to account for some reflexes and sensory phenomena, such as optical illusions. But even slightly more complex levels of mental experience have evaded our theories.
> We are quickly approaching the point when our traditional reasons for pleading ignorance – that we don’t have the right tools, that we need more data, that brains are complex and chaotic – will not account for our lack of explanations. Our techniques for seeing what the brain and its neurons are doing, at any given moment, get stronger every year.

**7. [AlphaGo Zero: Learning from scratch](https://deepmind.com/blog/alphago-zero-learning-scratch/)**
> However, for some problems this human knowledge may be too expensive, too unreliable or simply unavailable. As a result, a long-standing ambition of AI research is to bypass this step, creating algorithms that achieve superhuman performance in the most challenging domains with no human input. In our most [recent paper](https://www.nature.com/nature/journal/v550/n7676/full/nature24270.html), published in the journal Nature, we demonstrate a significant step towards this goal.
> The paper introduces AlphaGo Zero, the latest evolution of AlphaGo, the first computer program to defeat a world champion at the ancient Chinese game of Go. Zero is even more powerful and is arguably the strongest Go player in history.

**8. [GOOGLE'S LEARNING SOFTWARE LEARNS TO WRITE LEARNING SOFTWARE](https://www.wired.com/story/googles-learning-software-learns-to-write-learning-software/)**
> In a project called AutoML, Google’s researchers have taught machine-learning software to build machine-learning software. In some instances, what it comes up with is more powerful and efficient than the best systems the researchers themselves can design. Google says the system recently scored a record 82 percent at categorizing images by their content. On the harder task of marking the location of multiple objects in an image, an important task for augmented reality and autonomous robots, the auto-generated system scored 43 percent. The best human-built system scored 39 percent.


